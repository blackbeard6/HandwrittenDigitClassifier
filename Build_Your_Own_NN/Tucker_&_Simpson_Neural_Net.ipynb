{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entree Task: Implementing Your Own Neural Networks from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt # for graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Linear Layer \n",
    "Implement the forward and backward functions for a linear layer. Please read the requirement details for Task 1 in the code comment and in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, _m, _n):\n",
    "        '''\n",
    "        :param _m: _m is the input X hidden size\n",
    "        :param _n: _n is the output Y hidden size\n",
    "        '''\n",
    "        # \"Kaiming initialization\" is important for neural network to converge. The NN will not converge without it!\n",
    "        self.W = (np.random.uniform(low=-10000.0, high=10000.0, size = (_m, _n)))/10000.0*np.sqrt(6.0/ _m)\n",
    "        self.stored_X = None\n",
    "        self.W_grad = None #record the gradient of the weight\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        :param X: shape(X)[0] is batch size and shape(X)[1] is the #features\n",
    "         (1) Store the input X in stored_data for Backward.\n",
    "         (2) :return: X * weights\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        \n",
    "        #1\n",
    "        self.stored_X = X\n",
    "        \n",
    "        #2 \n",
    "        return X @ self.W\n",
    "        \n",
    "        ##########  Code end   ##########\n",
    "    \n",
    "    def backward(self, Y_grad):\n",
    "        '''\n",
    "        /* shape(output_grad)[0] is batch size and shape(output_grad)[1] is the # output features (shape(weight)[1])\n",
    "         * 1) Calculate the gradient of the output (the result of the Forward method) w.r.t. the **W** and store the product of the gradient and Y_grad in W_grad\n",
    "         * 2) Calculate the gradient of the output (the result of the Forward method) w.r.t. the **X** and return the product of the gradient and Y_grad\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        \n",
    "        #1\n",
    "        # calculate \"(partial L over partial Y = Y_grad) * \"(Partial Y over partial W )\"\n",
    "        self.W_grad = self.stored_X.T @ Y_grad \n",
    "        \n",
    "        #2\n",
    "        # calculate derrivative of the loss function \"partial L over partial Y\" = 2(Y^ - Y)\n",
    "        return Y_grad @ self.W.T\n",
    "        \n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 1: Linear Layer\n",
    "Check your linear forward and backward function implementations with numerical derivatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your gradient:  [[ 0.33865847 -0.36834135  1.68568181]]\n",
      "Numerical gradient: [[ 0.33865847 -0.36834135  1.68568181]]\n",
      "Error:  3.829436767688321e-12\n",
      "Correct backward. Congratulations!\n"
     ]
    }
   ],
   "source": [
    "#gradient check\n",
    "import copy\n",
    "#Random test\n",
    "n = 3\n",
    "m = 6\n",
    "Y_grad = np.random.rand(1, m)\n",
    "test_vector = np.random.rand(1, n)\n",
    "DELTA = 1e-6\n",
    "test_layer = LinearLayer(n, m)\n",
    "\n",
    "test_layer_1 = copy.deepcopy(test_layer)\n",
    "test_layer_2 = copy.deepcopy(test_layer)\n",
    "\n",
    "test_layer.forward(test_vector)\n",
    "Your_backward = test_layer.backward(Y_grad)\n",
    "\n",
    "cal_gradient = np.zeros((np.shape(test_vector)[0], np.shape(test_vector)[1]))\n",
    "for t_p in range(np.shape(test_vector)[0]):\n",
    "    for i in range(np.shape(test_vector)[1]):\n",
    "        test_vector_1 = copy.deepcopy(test_vector)\n",
    "        test_vector_2 = copy.deepcopy(test_vector)\n",
    "        test_vector_1[t_p][i] = test_vector_1[t_p][i] + DELTA\n",
    "        test_vector_2[t_p][i] = test_vector_2[t_p][i] - DELTA\n",
    "\n",
    "        cal_gradient[t_p][i] = np.sum(\n",
    "            (np.dot(Y_grad, np.transpose(test_layer_1.forward(test_vector_1) - test_layer_2.forward(test_vector_2))/(2*DELTA))))\n",
    "\n",
    "\n",
    "print('Your gradient: ',Your_backward)\n",
    "print('Numerical gradient:',cal_gradient)\n",
    "print('Error: ',abs(np.sum(Your_backward - cal_gradient)))\n",
    "if abs(np.sum(Your_backward - cal_gradient)) < 1e-4:\n",
    "    print('Correct backward. Congratulations!')\n",
    "else:\n",
    "    print('Wrong backawrd. Please check your implementation again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Non-Linear Activation\n",
    "Implement the forward and backward functions for a nonlinear layer. Please read the requirement details for Task 2 in the code comment and in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    #sigmoid layer\n",
    "    def __init__(self):\n",
    "        self.stored_X = None # Here we should store the input matrix X for Backward\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        /*\n",
    "         *  The input X matrix has the dimension [#samples, #features].\n",
    "         *  The output Y matrix has the same dimension as the input X.\n",
    "         *  You need to perform ReLU on each element of the input matrix to calculate the output matrix.\n",
    "         *  TODO: 1) Create an output matrix by going through each element in input and calculate relu=max(0,x) and\n",
    "         *  TODO: 2) Store the input X in self.stored_X for Backward.\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        #1\n",
    "        shape_X = np.zeros((X.shape[0], X.shape[1]))\n",
    "        func = lambda x: max(0, x)\n",
    "        Y = np.array(list(map(func, X.flatten()))).reshape((shape_X.shape[0], shape_X.shape[1]))\n",
    "\n",
    "        #2\n",
    "        self.stored_X = X\n",
    "        self.stored_X.reshape((shape_X.shape[0], shape_X.shape[1]))\n",
    "        \n",
    "        \n",
    "        return Y\n",
    "\n",
    "        ##########  Code end   ##########\n",
    "\n",
    "    def backward(self, Y_grad):\n",
    "        '''\n",
    "         /*  grad_relu(x)=1 if relu(x)=x\n",
    "         *  grad_relu(x)=0 if relu(x)=0\n",
    "         *\n",
    "         *  The input matrix has the name \"output_grad.\" The name is confusing (it is actually the input of the function). But the name follows the convension in PyTorch.\n",
    "         *  The output matrix has the same dimension as input.\n",
    "         *  The output matrix is calculated as grad_relu(stored_X)*Y_grad.\n",
    "         *  TODO: returns the output matrix calculated above\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        \n",
    "        #1\n",
    "        def grad_relu(x):\n",
    "            if x > 0:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "        shape_stored_X = np.zeros((self.stored_X.shape[0], self.stored_X.shape[1]))\n",
    "        \n",
    "        func = lambda x: grad_relu(x)\n",
    "        grad_relu_X = np.array(list(map(func, self.stored_X.flatten())))\n",
    "        grad_relu_X_2D = grad_relu_X.reshape(shape_stored_X.shape[0], shape_stored_X.shape[1])\n",
    "    \n",
    "        return grad_relu_X_2D * Y_grad\n",
    "\n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 2: ReLU \n",
    "Check your ReLU forward and backward functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your gradient:  [[0.4833622  0.60281078 0.64823861]]\n",
      "Numerical gradient: [[0.4833622  0.60281078 0.64823861]]\n",
      "Error:  4.640504647213106e-11\n",
      "Correct backward. Congratulations!\n"
     ]
    }
   ],
   "source": [
    "#gradient check\n",
    "import copy\n",
    "#Random test\n",
    "n = 3\n",
    "Y_grad = np.random.rand(1, n)\n",
    "test_vector = np.random.rand(1, n)\n",
    "DELTA = 1e-6\n",
    "test_layer = ReLU()\n",
    "\n",
    "test_layer_1 = copy.deepcopy(test_layer)\n",
    "test_layer_2 = copy.deepcopy(test_layer)\n",
    "\n",
    "test_layer.forward(test_vector)\n",
    "Your_backward = test_layer.backward(Y_grad)\n",
    "\n",
    "cal_gradient = np.zeros((np.shape(test_vector)[0], np.shape(test_vector)[1]))\n",
    "for t_p in range(np.shape(test_vector)[0]):\n",
    "    for i in range(np.shape(test_vector)[1]):\n",
    "        test_vector_1 = copy.deepcopy(test_vector)\n",
    "        test_vector_2 = copy.deepcopy(test_vector)\n",
    "        test_vector_1[t_p][i] = test_vector_1[t_p][i] + DELTA\n",
    "        test_vector_2[t_p][i] = test_vector_2[t_p][i] - DELTA\n",
    "\n",
    "        cal_gradient[t_p][i] = np.sum(\n",
    "            (np.dot(Y_grad, np.transpose(test_layer_1.forward(test_vector_1) - test_layer_2.forward(test_vector_2))/(2*DELTA))))\n",
    "\n",
    "\n",
    "print('Your gradient: ',Your_backward)\n",
    "print('Numerical gradient:',cal_gradient)\n",
    "print('Error: ',abs(np.sum(Your_backward - cal_gradient)))\n",
    "if abs(np.sum(Your_backward - cal_gradient)) < 1e-4:\n",
    "    print('Correct backward. Congratulations!')\n",
    "else:\n",
    "    print('Wrong backawrd. Please check your implementation again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Loss Function\n",
    "Implement the MSE loss function and its backward derivative. Please read the requirement details for Task 3 in the code comment and in the pdf document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    # cross entropy loss\n",
    "    # return the mse loss mean(y_j-y_pred_i)^2\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stored_diff = None\n",
    "    def forward(self, prediction, groundtruth):\n",
    "        '''\n",
    "        /*  TODO: 1) Calculate stored_data=pred-truth\n",
    "         *  TODO: 2) Calculate the MSE loss as the squared sum of all the elements in the stored_data divided by the number of elements, i.e., MSE(pred, truth) = ||pred-truth||^2 / N, with N as the total number of elements in the matrix\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        \n",
    "        \n",
    "        #1\n",
    "        self.stored_diff = prediction - groundtruth\n",
    "        \n",
    "        #2\n",
    "        MSE_loss = np.sum(np.square(self.stored_diff)) / prediction.size\n",
    "        \n",
    "        return MSE_loss\n",
    "\n",
    "        ##########  Code end   ##########\n",
    "\n",
    "    # return the gradient of the input data\n",
    "    def backward(self):\n",
    "        '''\n",
    "        /* TODO: return the gradient matrix of the MSE loss\n",
    "         * The output matrix has the same dimension as the stored_data (make sure you have stored the (pred-truth) in stored_data in your forward function!)\n",
    "         * Each element (i,j) of the output matrix is calculated as grad(i,j)=2(pred(i,j)-truth(i,j))/N\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        \n",
    "        #1\n",
    "        MSE_grad_mtx = np.zeros((self.stored_diff.shape[0], self.stored_diff.shape[1]))\n",
    "        \n",
    "        for i in range (0, MSE_grad_mtx.shape[0]):\n",
    "            for j in range(0, MSE_grad_mtx.shape[1]):\n",
    "                MSE_grad_mtx[i, j] = 2*(self.stored_diff[i,j]) / MSE_grad_mtx.size\n",
    "        \n",
    "        return MSE_grad_mtx\n",
    "        \n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Network Architecture\n",
    "Implement your own neural network architecture. Please read the requirement for Task 4 in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers_arch):\n",
    "        '''\n",
    "        /*  TODO: 1) Initialize the array for input layers with the proper feature sizes specified in the input vector.\n",
    "         * For the linear layer, in each pair (in_size, out_size), the in_size is the feature size of the previous layer and the out_size is the feature size of the output (that goes to the next layer)\n",
    "         * In the linear layer, the weight should have the shape (in_size, out_size).\n",
    "         \n",
    "         *  For example, if layers_arch = [['Linear', (256, 128)], ['ReLU'], ['Linear', (128, 64)], ['ReLU'], ['Linear', (64, 32)]],\n",
    "       * \t\t\t\t\t\t\t then there are three linear layers whose weights are with shapes (256, 128), (128, 64), (64, 32),\n",
    "       * \t\t\t\t\t\t\t and there are two non-linear layers.\n",
    "         *  Attention: * The output feature size of the linear layer i should always equal to the input feature size of the linear layer i+1.\n",
    "       */\n",
    "        '''\n",
    "       \n",
    "        ########## Code start  ##########\n",
    "        \n",
    "        #1\n",
    "        self.layers = []\n",
    "        for j in range(0, len(layers_arch)):\n",
    "            if layers_arch[j][0] == \"Linear\":\n",
    "                self.layers.append(LinearLayer(layers_arch[j][1][0], layers_arch[j][1][1]))\n",
    "            elif layers_arch[j][0] == \"ReLU\":\n",
    "                self.layers.append(ReLU())\n",
    "                \n",
    "                \n",
    "        ##########  Code end   ##########\n",
    "        \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        /*\n",
    "         * TODO: propagate the input data for the first linear layer throught all the layers in the network and return the output of the last linear layer.\n",
    "         * For implementation, you need to write a for-loop to propagate the input from the first layer to the last layer (before the loss function) by going through the forward functions of all the layers.\n",
    "         * For example, for a network with k linear layers and k-1 activation layers, the data flow is:\n",
    "         * linear[0] -> activation[0] -> linear[1] ->activation[1] -> ... -> linear[k-2] -> activation[k-2] -> linear[k-1]\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        #1\n",
    "        for i in range(0, len(self.layers)):\n",
    "            if i == 0:\n",
    "                out_layers = self.layers[i].forward(X)\n",
    "            else:\n",
    "                out_layers = self.layers[i].forward(out_layers)\n",
    "        return out_layers\n",
    "\n",
    "        ##########  Code end   ##########\n",
    "\n",
    "    def backward(self, Y_grad):\n",
    "        '''\n",
    "        /* Propagate the gradient from the last layer to the first layer by going through the backward functions of all the layers.\n",
    "         * TODO: propagate the gradient of the output (we got from the Forward method) back throught the network and return the gradient of the first layer.\n",
    "\n",
    "         * Notice: We should use the chain rule for the backward.\n",
    "         * Notice: The order is opposite to the forward.\n",
    "         */\n",
    "        '''\n",
    "        \n",
    "        ########## Code start  ##########\n",
    "        #1\n",
    "        for i in range(len(self.layers) -1, -1, -1):\n",
    "            if i == (len(self.layers) -1):\n",
    "                out_layers = self.layers[i].backward(Y_grad)\n",
    "            else:\n",
    "                out_layers = self.layers[i].backward(out_layers)\n",
    "        return out_layers\n",
    "\n",
    "    \n",
    "        ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 3: Regression Network\n",
    "Check your network implementation with a simple regression task. Here we also provide you a sample implementation for the gradient descent algorithm, which you will find useful for your own Classifier implementation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor:\n",
    "    #Classifier\n",
    "    def __init__(self, layers_arch, data_function, learning_rate = 3e-3, batch_size = 32, max_epoch = 200):\n",
    "\n",
    "        input_feature_size = 2\n",
    "        output_feature_size = 2\n",
    "\n",
    "        self.train_data = []\n",
    "        self.train_label = []\n",
    "        self.test_data = []\n",
    "        self.test_label = []\n",
    "\n",
    "        self.data_function = data_function\n",
    "        \n",
    "        self.layers_arch = layers_arch\n",
    "        self.net = Network(layers_arch)\n",
    "        self.loss_function = MSELoss()\n",
    "\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def dataloader(self):\n",
    "        \n",
    "        '''\n",
    "        We randomly generate the mapping: (x)->(x^3+x^2 + 1)\n",
    "        '''\n",
    "        self.train_data = np.zeros((1000,1))\n",
    "        self.train_label = np.zeros((1000, 1))\n",
    "\n",
    "        for i in range(1000):\n",
    "            self.train_data[i][0] = np.random.uniform(low=0.0, high=10000.0)/10000.0\n",
    "            self.train_label[i][0] = self.data_function(self.train_data[i][0])\n",
    "\n",
    "        self.test_data = np.zeros((200, 1))\n",
    "        self.test_label = np.zeros((200, 1))\n",
    "\n",
    "        for i in range(200):\n",
    "            self.test_data[i][0] = np.random.uniform(low=-0.0, high=10000.0) / 10000.0\n",
    "            self.test_label[i][0] = self.data_function(self.test_data[i][0])\n",
    "\n",
    "\n",
    "\n",
    "    def Train_One_Epoch(self):\n",
    "        '''\n",
    "        Here we train the network using gradient descent\n",
    "        '''\n",
    "        loss = 0\n",
    "        n_loop = int(math.ceil(len(self.train_data)/self.batch_size))\n",
    "\n",
    "        for i in range(n_loop):\n",
    "            batch_data = self.train_data[i * self.batch_size : (i+1)*self.batch_size]\n",
    "            batch_label = self.train_label[i * self.batch_size : (i+1)*self.batch_size]\n",
    "            \n",
    "            '''\n",
    "            /*  Forward the data to the network.\n",
    "             *  Forward the result to the loss function.\n",
    "             *  Backward.\n",
    "             *  Update the weights with weight gradients.\n",
    "             *  Do not forget the learning rate!\n",
    "             */\n",
    "            '''\n",
    "            \n",
    "            ########## Sample code  ##########\n",
    "            prediction = self.net.forward(batch_data)\n",
    "            loss += self.loss_function.forward(prediction, batch_label)\n",
    "\n",
    "            pred_grad = self.loss_function.backward()\n",
    "            self.net.backward(pred_grad)\n",
    "            for i in range(len(self.layers_arch)):\n",
    "        \n",
    "                if self.layers_arch[i][0] == 'Linear':\n",
    "                    self.net.layers[i].W -= self.net.layers[i].W_grad * self.learning_rate\n",
    "            ##########  Sample code ##########\n",
    "            \n",
    "        return loss/n_loop\n",
    "\n",
    "    def Test(self):\n",
    "        prediction = self.net.forward(self.test_data)\n",
    "        loss = self.loss_function.forward(prediction, self.test_label)\n",
    "        return loss\n",
    "\n",
    "    def Train(self):\n",
    "        x = np.zeros((self.max_epoch, 1))\n",
    "        y = np.zeros((self.max_epoch, 1))\n",
    "        self.dataloader()\n",
    "        for i in range(self.max_epoch):\n",
    "            train_loss = self.Train_One_Epoch()\n",
    "            test_loss = self.Test()\n",
    "            print(\"Epoch: \", str(i+1), \"/\", str(self.max_epoch), \" | Train loss: \", train_loss, \" | Test loss : \", test_loss)\n",
    "            x[i] = i\n",
    "            y[i] = test_loss\n",
    "        plt.figure()\n",
    "        plt.title(\"Regression Network Test Loss Converging to Zero\")\n",
    "        plt.scatter(x, y)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 / 200  | Train loss:  4.030777015886711  | Test loss :  3.87269047478234\n",
      "Epoch:  2 / 200  | Train loss:  3.8030717028655467  | Test loss :  3.6544790256993656\n",
      "Epoch:  3 / 200  | Train loss:  3.5914920025953405  | Test loss :  3.4512386678888607\n",
      "Epoch:  4 / 200  | Train loss:  3.3940071156969904  | Test loss :  3.2611344070781803\n",
      "Epoch:  5 / 200  | Train loss:  3.2089321986774095  | Test loss :  3.0826410612519783\n",
      "Epoch:  6 / 200  | Train loss:  3.0348648888759038  | Test loss :  2.914487377696479\n",
      "Epoch:  7 / 200  | Train loss:  2.8706350688126228  | Test loss :  2.755611553276998\n",
      "Epoch:  8 / 200  | Train loss:  2.715264684642906  | Test loss :  2.6051254583305576\n",
      "Epoch:  9 / 200  | Train loss:  2.56793527099886  | Test loss :  2.4622855641610126\n",
      "Epoch:  10 / 200  | Train loss:  2.427961435877007  | Test loss :  2.3264690797558645\n",
      "Epoch:  11 / 200  | Train loss:  2.2947689961873365  | Test loss :  2.19715417298586\n",
      "Epoch:  12 / 200  | Train loss:  2.16787677559363  | Test loss :  2.0739034246368133\n",
      "Epoch:  13 / 200  | Train loss:  2.0468813145001343  | Test loss :  1.9563498672997885\n",
      "Epoch:  14 / 200  | Train loss:  1.93144392041021  | Test loss :  1.8441851143017676\n",
      "Epoch:  15 / 200  | Train loss:  1.8212796214386793  | Test loss :  1.7371491997819701\n",
      "Epoch:  16 / 200  | Train loss:  1.7161476878386999  | Test loss :  1.63502183915591\n",
      "Epoch:  17 / 200  | Train loss:  1.6158434641059818  | Test loss :  1.5376148863499255\n",
      "Epoch:  18 / 200  | Train loss:  1.5201913134020382  | Test loss :  1.4447658152737362\n",
      "Epoch:  19 / 200  | Train loss:  1.4290385209796186  | Test loss :  1.3563320916728407\n",
      "Epoch:  20 / 200  | Train loss:  1.3422500371836938  | Test loss :  1.2721863305014667\n",
      "Epoch:  21 / 200  | Train loss:  1.2597039658512514  | Test loss :  1.1922121553721023\n",
      "Epoch:  22 / 200  | Train loss:  1.181287722405358  | Test loss :  1.1163006921046879\n",
      "Epoch:  23 / 200  | Train loss:  1.1068947991042595  | Test loss :  1.0443476392243016\n",
      "Epoch:  24 / 200  | Train loss:  1.0364220839525204  | Test loss :  0.9762508655077491\n",
      "Epoch:  25 / 200  | Train loss:  0.9697676856858916  | Test loss :  0.9119084892426301\n",
      "Epoch:  26 / 200  | Train loss:  0.9068292208210783  | Test loss :  0.8512173964807798\n",
      "Epoch:  27 / 200  | Train loss:  0.8475025207019088  | Test loss :  0.7940721568661414\n",
      "Epoch:  28 / 200  | Train loss:  0.7916807173497364  | Test loss :  0.7403642961165725\n",
      "Epoch:  29 / 200  | Train loss:  0.7392536672145458  | Test loss :  0.6899818843667314\n",
      "Epoch:  30 / 200  | Train loss:  0.6901076720088438  | Test loss :  0.6428093996734191\n",
      "Epoch:  31 / 200  | Train loss:  0.6441254559876453  | Test loss :  0.5987278263008827\n",
      "Epoch:  32 / 200  | Train loss:  0.601186359532507  | Test loss :  0.557614948118901\n",
      "Epoch:  33 / 200  | Train loss:  0.5611667098485329  | Test loss :  0.5193457986672954\n",
      "Epoch:  34 / 200  | Train loss:  0.523940331066023  | Test loss :  0.48379323120988393\n",
      "Epoch:  35 / 200  | Train loss:  0.48937915806951615  | Test loss :  0.4508285744083071\n",
      "Epoch:  36 / 200  | Train loss:  0.45738254778381965  | Test loss :  0.4208478611053895\n",
      "Epoch:  37 / 200  | Train loss:  0.42995503173528915  | Test loss :  0.39602943131394125\n",
      "Epoch:  38 / 200  | Train loss:  0.40571560003509405  | Test loss :  0.37283406325829865\n",
      "Epoch:  39 / 200  | Train loss:  0.3830747259887025  | Test loss :  0.3511960166009079\n",
      "Epoch:  40 / 200  | Train loss:  0.3619656791752061  | Test loss :  0.331047674350607\n",
      "Epoch:  41 / 200  | Train loss:  0.342320120089603  | Test loss :  0.3123201314095356\n",
      "Epoch:  42 / 200  | Train loss:  0.32406867799345507  | Test loss :  0.29494375711943616\n",
      "Epoch:  43 / 200  | Train loss:  0.30714149920643413  | Test loss :  0.27884872452334714\n",
      "Epoch:  44 / 200  | Train loss:  0.2914687592639727  | Test loss :  0.2639655005779373\n",
      "Epoch:  45 / 200  | Train loss:  0.2769811338726605  | Test loss :  0.2502252930330425\n",
      "Epoch:  46 / 200  | Train loss:  0.2636102250442455  | Test loss :  0.2375604510992696\n",
      "Epoch:  47 / 200  | Train loss:  0.2512889401457056  | Test loss :  0.22590481832052867\n",
      "Epoch:  48 / 200  | Train loss:  0.23995182283938846  | Test loss :  0.21519403723292524\n",
      "Epoch:  49 / 200  | Train loss:  0.22953533598428358  | Test loss :  0.20536580640917787\n",
      "Epoch:  50 / 200  | Train loss:  0.21997809751521105  | Test loss :  0.19636009135062438\n",
      "Epoch:  51 / 200  | Train loss:  0.2112210711062371  | Test loss :  0.18811929139530473\n",
      "Epoch:  52 / 200  | Train loss:  0.20320771405896865  | Test loss :  0.18058836536439352\n",
      "Epoch:  53 / 200  | Train loss:  0.19588408534159765  | Test loss :  0.1737149190786992\n",
      "Epoch:  54 / 200  | Train loss:  0.18919891705063302  | Test loss :  0.16744925815360648\n",
      "Epoch:  55 / 200  | Train loss:  0.18310365278687754  | Test loss :  0.16174440963863126\n",
      "Epoch:  56 / 200  | Train loss:  0.1775524565450825  | Test loss :  0.156556116121941\n",
      "Epoch:  57 / 200  | Train loss:  0.17250219572840408  | Test loss :  0.15184280588662438\n",
      "Epoch:  58 / 200  | Train loss:  0.1679124018301508  | Test loss :  0.147565542599866\n",
      "Epoch:  59 / 200  | Train loss:  0.16374521219179533  | Test loss :  0.14368795785357769\n",
      "Epoch:  60 / 200  | Train loss:  0.15996529606242782  | Test loss :  0.14017616966945798\n",
      "Epoch:  61 / 200  | Train loss:  0.15653976796412572  | Test loss :  0.13699868984554173\n",
      "Epoch:  62 / 200  | Train loss:  0.15343809112209222  | Test loss :  0.1341263227662613\n",
      "Epoch:  63 / 200  | Train loss:  0.15063197345826873  | Test loss :  0.13153205803341042\n",
      "Epoch:  64 / 200  | Train loss:  0.14809525838132204  | Test loss :  0.12919095900923372\n",
      "Epoch:  65 / 200  | Train loss:  0.14580381234169357  | Test loss :  0.12708004910161896\n",
      "Epoch:  66 / 200  | Train loss:  0.14373541086363928  | Test loss :  0.12517819737016672\n",
      "Epoch:  67 / 200  | Train loss:  0.14186962452132712  | Test loss :  0.1234660047945454\n",
      "Epoch:  68 / 200  | Train loss:  0.14018770609639217  | Test loss :  0.12192569232571994\n",
      "Epoch:  69 / 200  | Train loss:  0.1386724799420739  | Test loss :  0.1205409916380706\n",
      "Epoch:  70 / 200  | Train loss:  0.137308234385536  | Test loss :  0.11929703931703588\n",
      "Epoch:  71 / 200  | Train loss:  0.1360806178257412  | Test loss :  0.11818027505292211\n",
      "Epoch:  72 / 200  | Train loss:  0.13497653902934117  | Test loss :  0.11717834426667646\n",
      "Epoch:  73 / 200  | Train loss:  0.13398407199093632  | Test loss :  0.1162800054670015\n",
      "Epoch:  74 / 200  | Train loss:  0.13309236560593135  | Test loss :  0.11547504252921163\n",
      "Epoch:  75 / 200  | Train loss:  0.13229155830295866  | Test loss :  0.11475418199350258\n",
      "Epoch:  76 / 200  | Train loss:  0.13157269769720983  | Test loss :  0.11410901540249124\n",
      "Epoch:  77 / 200  | Train loss:  0.13092766525463292  | Test loss :  0.11353192663359472\n",
      "Epoch:  78 / 200  | Train loss:  0.13034910589842663  | Test loss :  0.11301602412964527\n",
      "Epoch:  79 / 200  | Train loss:  0.1298303624421968  | Test loss :  0.11255507788971078\n",
      "Epoch:  80 / 200  | Train loss:  0.12936541469717397  | Test loss :  0.11214346105006384\n",
      "Epoch:  81 / 200  | Train loss:  0.12894882307273775  | Test loss :  0.1117760958613827\n",
      "Epoch:  82 / 200  | Train loss:  0.12857567646894016  | Test loss :  0.11144840385140417\n",
      "Epoch:  83 / 200  | Train loss:  0.12824154424566075  | Test loss :  0.11115625995131667\n",
      "Epoch:  84 / 200  | Train loss:  0.1279424320444295  | Test loss :  0.11089595035822349\n",
      "Epoch:  85 / 200  | Train loss:  0.1276747412349239  | Test loss :  0.11066413390415636\n",
      "Epoch:  86 / 200  | Train loss:  0.12743523175785673  | Test loss :  0.11045780670361698\n",
      "Epoch:  87 / 200  | Train loss:  0.12722098813870716  | Test loss :  0.11027426985579879\n",
      "Epoch:  88 / 200  | Train loss:  0.127029388451888  | Test loss :  0.11011109998391413\n",
      "Epoch:  89 / 200  | Train loss:  0.12685807602193508  | Test loss :  0.10996612240192004\n",
      "Epoch:  90 / 200  | Train loss:  0.12670493365669921  | Test loss :  0.10983738670797764\n",
      "Epoch:  91 / 200  | Train loss:  0.12656806021690942  | Test loss :  0.10972314461383377\n",
      "Epoch:  92 / 200  | Train loss:  0.12644574933654387  | Test loss :  0.10962182982968123\n",
      "Epoch:  93 / 200  | Train loss:  0.1263364701189005  | Test loss :  0.10953203983468757\n",
      "Epoch:  94 / 200  | Train loss:  0.12623884964389628  | Test loss :  0.10945251937408212\n",
      "Epoch:  95 / 200  | Train loss:  0.12615165713274512  | Test loss :  0.10938214553429589\n",
      "Epoch:  96 / 200  | Train loss:  0.12607378962663443  | Test loss :  0.10931991425803263\n",
      "Epoch:  97 / 200  | Train loss:  0.1260042590462264  | Test loss :  0.10926492817121222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  98 / 200  | Train loss:  0.12594218050865832  | Test loss :  0.10921638560340131\n",
      "Epoch:  99 / 200  | Train loss:  0.12588676178815839  | Test loss :  0.10917357069257524\n",
      "Epoch:  100 / 200  | Train loss:  0.12583729381537057  | Test loss :  0.1091358444738066\n",
      "Epoch:  101 / 200  | Train loss:  0.12579314211897874  | Test loss :  0.10910263685973005\n",
      "Epoch:  102 / 200  | Train loss:  0.12575373912121315  | Test loss :  0.1090734394283794\n",
      "Epoch:  103 / 200  | Train loss:  0.12571857720631255  | Test loss :  0.10904779894123241\n",
      "Epoch:  104 / 200  | Train loss:  0.12568720248800222  | Test loss :  0.10902531152104025\n",
      "Epoch:  105 / 200  | Train loss:  0.12565920920854493  | Test loss :  0.10900561742527244\n",
      "Epoch:  106 / 200  | Train loss:  0.12563423470794072  | Test loss :  0.10898839635679383\n",
      "Epoch:  107 / 200  | Train loss:  0.12561195490741492  | Test loss :  0.10897336325872792\n",
      "Epoch:  108 / 200  | Train loss:  0.12559208025645846  | Test loss :  0.10896026454537068\n",
      "Epoch:  109 / 200  | Train loss:  0.12557435209739568  | Test loss :  0.10894887472552829\n",
      "Epoch:  110 / 200  | Train loss:  0.12555853940577766  | Test loss :  0.10893899337878102\n",
      "Epoch:  111 / 200  | Train loss:  0.1255444358688547  | Test loss :  0.10893044244895256\n",
      "Epoch:  112 / 200  | Train loss:  0.12553185726799504  | Test loss :  0.10892306382250984\n",
      "Epoch:  113 / 200  | Train loss:  0.12552063913421657  | Test loss :  0.10891671716275876\n",
      "Epoch:  114 / 200  | Train loss:  0.12551063464899742  | Test loss :  0.10891127797355787\n",
      "Epoch:  115 / 200  | Train loss:  0.12550171276526453  | Test loss :  0.1089066358688688\n",
      "Epoch:  116 / 200  | Train loss:  0.1254937565259375  | Test loss :  0.10890269302681617\n",
      "Epoch:  117 / 200  | Train loss:  0.1254866615596541  | Test loss :  0.10889936280906501\n",
      "Epoch:  118 / 200  | Train loss:  0.12548033473534229  | Test loss :  0.1088965685282558\n",
      "Epoch:  119 / 200  | Train loss:  0.1254746929591462  | Test loss :  0.10889424234798428\n",
      "Epoch:  120 / 200  | Train loss:  0.12546966209888197  | Test loss :  0.10889232430139184\n",
      "Epoch:  121 / 200  | Train loss:  0.12546517602270452  | Test loss :  0.10889076141585746\n",
      "Epoch:  122 / 200  | Train loss:  0.12546117574002538  | Test loss :  0.10888950693256665\n",
      "Epoch:  123 / 200  | Train loss:  0.12545760863394698  | Test loss :  0.10888851961089102\n",
      "Epoch:  124 / 200  | Train loss:  0.12545442777558383  | Test loss :  0.10888776310855505\n",
      "Epoch:  125 / 200  | Train loss:  0.12545159131163433  | Test loss :  0.10888720542950397\n",
      "Epoch:  126 / 200  | Train loss:  0.12544906191746186  | Test loss :  0.10888681843223172\n",
      "Epoch:  127 / 200  | Train loss:  0.125446806308749  | Test loss :  0.10888657739208489\n",
      "Epoch:  128 / 200  | Train loss:  0.12544479480551055  | Test loss :  0.10888646061174052\n",
      "Epoch:  129 / 200  | Train loss:  0.12544300094290267  | Test loss :  0.10888644907466645\n",
      "Epoch:  130 / 200  | Train loss:  0.12544140112384614  | Test loss :  0.10888652613692183\n",
      "Epoch:  131 / 200  | Train loss:  0.12543997430900666  | Test loss :  0.10888667725314714\n",
      "Epoch:  132 / 200  | Train loss:  0.1254387017401449  | Test loss :  0.10888688973303419\n",
      "Epoch:  133 / 200  | Train loss:  0.12543756669326858  | Test loss :  0.10888715252496141\n",
      "Epoch:  134 / 200  | Train loss:  0.12543655425839678  | Test loss :  0.10888745602383441\n",
      "Epoch:  135 / 200  | Train loss:  0.12543565114308572  | Test loss :  0.10888779190048752\n",
      "Epoch:  136 / 200  | Train loss:  0.12543484549716535  | Test loss :  0.10888815295028655\n",
      "Epoch:  137 / 200  | Train loss:  0.1254341267564107  | Test loss :  0.10888853295882651\n",
      "Epoch:  138 / 200  | Train loss:  0.12543348550311123  | Test loss :  0.10888892658284444\n",
      "Epoch:  139 / 200  | Train loss:  0.12543291334172088  | Test loss :  0.10888932924467142\n",
      "Epoch:  140 / 200  | Train loss:  0.12543240278796414  | Test loss :  0.10888973703872776\n",
      "Epoch:  141 / 200  | Train loss:  0.12543194716994846  | Test loss :  0.10889014664872927\n",
      "Epoch:  142 / 200  | Train loss:  0.1254315405399876  | Test loss :  0.10889055527441532\n",
      "Epoch:  143 / 200  | Train loss:  0.1254311775959802  | Test loss :  0.10889096056674034\n",
      "Epoch:  144 / 200  | Train loss:  0.12543085361131104  | Test loss :  0.10889136057058461\n",
      "Epoch:  145 / 200  | Train loss:  0.1254305643723543  | Test loss :  0.10889175367414464\n",
      "Epoch:  146 / 200  | Train loss:  0.12543030612275616  | Test loss :  0.10889213856425328\n",
      "Epoch:  147 / 200  | Train loss:  0.12543007551376428  | Test loss :  0.10889251418696493\n",
      "Epoch:  148 / 200  | Train loss:  0.12542986955994803  | Test loss :  0.10889287971281039\n",
      "Epoch:  149 / 200  | Train loss:  0.12542968559972692  | Test loss :  0.10889323450619554\n",
      "Epoch:  150 / 200  | Train loss:  0.12542952126018533  | Test loss :  0.10889357809847235\n",
      "Epoch:  151 / 200  | Train loss:  0.12542937442570948  | Test loss :  0.1088939101642655\n",
      "Epoch:  152 / 200  | Train loss:  0.1254292432100315  | Test loss :  0.10889423050068249\n",
      "Epoch:  153 / 200  | Train loss:  0.12542912593131167  | Test loss :  0.10889453900907688\n",
      "Epoch:  154 / 200  | Train loss:  0.1254290210899285  | Test loss :  0.10889483567907128\n",
      "Epoch:  155 / 200  | Train loss:  0.12542892734868305  | Test loss :  0.10889512057457913\n",
      "Epoch:  156 / 200  | Train loss:  0.12542884351515493  | Test loss :  0.10889539382159306\n",
      "Epoch:  157 / 200  | Train loss:  0.1254287685259769  | Test loss :  0.10889565559753452\n",
      "Epoch:  158 / 200  | Train loss:  0.12542870143281915  | Test loss :  0.10889590612198173\n",
      "Epoch:  159 / 200  | Train loss:  0.12542864138989707  | Test loss :  0.10889614564861372\n",
      "Epoch:  160 / 200  | Train loss:  0.12542858764283812  | Test loss :  0.10889637445822657\n",
      "Epoch:  161 / 200  | Train loss:  0.12542853951875882  | Test loss :  0.10889659285269429\n",
      "Epoch:  162 / 200  | Train loss:  0.12542849641742085  | Test loss :  0.10889680114976132\n",
      "Epoch:  163 / 200  | Train loss:  0.12542845780334888  | Test loss :  0.10889699967856595\n",
      "Epoch:  164 / 200  | Train loss:  0.1254284231988055  | Test loss :  0.10889718877580634\n",
      "Epoch:  165 / 200  | Train loss:  0.1254283921775299  | Test loss :  0.10889736878247033\n",
      "Epoch:  166 / 200  | Train loss:  0.12542836435915725  | Test loss :  0.10889754004105914\n",
      "Epoch:  167 / 200  | Train loss:  0.1254283394042449  | Test loss :  0.10889770289324399\n",
      "Epoch:  168 / 200  | Train loss:  0.1254283170098391  | Test loss :  0.10889785767790039\n",
      "Epoch:  169 / 200  | Train loss:  0.12542829690552373  | Test loss :  0.10889800472947302\n",
      "Epoch:  170 / 200  | Train loss:  0.1254282788498983  | Test loss :  0.10889814437662754\n",
      "Epoch:  171 / 200  | Train loss:  0.12542826262743842  | Test loss :  0.108898276941153\n",
      "Epoch:  172 / 200  | Train loss:  0.12542824804569783  | Test loss :  0.10889840273708094\n",
      "Epoch:  173 / 200  | Train loss:  0.12542823493281405  | Test loss :  0.10889852206999247\n",
      "Epoch:  174 / 200  | Train loss:  0.12542822313528465  | Test loss :  0.10889863523648753\n",
      "Epoch:  175 / 200  | Train loss:  0.1254282125159853  | Test loss :  0.10889874252379389\n",
      "Epoch:  176 / 200  | Train loss:  0.12542820295240276  | Test loss :  0.10889884420949576\n",
      "Epoch:  177 / 200  | Train loss:  0.12542819433505972  | Test loss :  0.10889894056136519\n",
      "Epoch:  178 / 200  | Train loss:  0.12542818656611032  | Test loss :  0.1088990318372802\n",
      "Epoch:  179 / 200  | Train loss:  0.12542817955808766  | Test loss :  0.10889911828521698\n",
      "Epoch:  180 / 200  | Train loss:  0.1254281732327881  | Test loss :  0.10889920014330393\n",
      "Epoch:  181 / 200  | Train loss:  0.12542816752027522  | Test loss :  0.1088992776399279\n",
      "Epoch:  182 / 200  | Train loss:  0.1254281623579923  | Test loss :  0.10889935099388297\n",
      "Epoch:  183 / 200  | Train loss:  0.12542815768997095  | Test loss :  0.1088994204145546\n",
      "Epoch:  184 / 200  | Train loss:  0.12542815346612482  | Test loss :  0.10889948610213235\n",
      "Epoch:  185 / 200  | Train loss:  0.1254281496416203  | Test loss :  0.10889954824784515\n",
      "Epoch:  186 / 200  | Train loss:  0.1254281461763149  | Test loss :  0.10889960703421395\n",
      "Epoch:  187 / 200  | Train loss:  0.12542814303425626  | Test loss :  0.10889966263531785\n",
      "Epoch:  188 / 200  | Train loss:  0.12542814018323536  | Test loss :  0.10889971521706958\n",
      "Epoch:  189 / 200  | Train loss:  0.125428137594388  | Test loss :  0.10889976493749731\n",
      "Epoch:  190 / 200  | Train loss:  0.12542813524183913  | Test loss :  0.10889981194702997\n",
      "Epoch:  191 / 200  | Train loss:  0.12542813310238513  | Test loss :  0.10889985638878377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  192 / 200  | Train loss:  0.12542813115521118  | Test loss :  0.10889989839884805\n",
      "Epoch:  193 / 200  | Train loss:  0.12542812938163814  | Test loss :  0.10889993810656878\n",
      "Epoch:  194 / 200  | Train loss:  0.12542812776489698  | Test loss :  0.10889997563482826\n",
      "Epoch:  195 / 200  | Train loss:  0.12542812628992764  | Test loss :  0.10890001110032006\n",
      "Epoch:  196 / 200  | Train loss:  0.12542812494319888  | Test loss :  0.10890004461381793\n",
      "Epoch:  197 / 200  | Train loss:  0.12542812371254816  | Test loss :  0.10890007628043864\n",
      "Epoch:  198 / 200  | Train loss:  0.12542812258703787  | Test loss :  0.10890010619989741\n",
      "Epoch:  199 / 200  | Train loss:  0.12542812155682762  | Test loss :  0.10890013446675585\n",
      "Epoch:  200 / 200  | Train loss:  0.1254281206130596  | Test loss :  0.10890016117066213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10890016117066213"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_path = './MNIST_Sub/train_data.txt'\n",
    "train_labels_path = './MNIST_Sub/train_labels.txt'\n",
    "test_data_path = './MNIST_Sub/test_data.txt'\n",
    "test_labels_path = './MNIST_Sub/test_labels.txt'\n",
    "\n",
    "# regressor\n",
    "regressor_layers_arch = [['Linear', (1, 16)], ['ReLU'], ['Linear', (16, 16)], ['ReLU'], ['Linear', (16, 1)]]\n",
    "def data_function(x):\n",
    "    return np.power(x,3) + pow(x,2) + 1\n",
    "regressor = Regressor(regressor_layers_arch, data_function, learning_rate = 1e-4, batch_size = 32, max_epoch = 200)\n",
    "regressor.Train()\n",
    "\n",
    "regressor.Test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Classfication Network\n",
    "Implement your own classifier with gradient descent. Please read the requirement for Task 5 in the pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_Hot_Encode(labels, classes = 10):\n",
    "    '''\n",
    "    /*  Make the labels one-hot.\n",
    "     *  For example, if there are 5 classes {0, 1, 2, 3, 4} then\n",
    "     *  [0, 2, 4] -> [[1, 0, 0, 0, 0],\n",
    "     * \t\t\t\t\t\t\t\t[0, 0, 1, 0, 0],\n",
    "     * \t\t\t\t\t\t\t\t[0, 0, 0, 0, 1]]\n",
    "     */\n",
    "    '''\n",
    "    \n",
    "    ########## Code start  ##########\n",
    "    hot = np.zeros((labels.size, classes))\n",
    "    \n",
    "    for i in range (0, labels.size - 1):\n",
    "        hot[i][labels[i]] = 1\n",
    "        \n",
    "    return hot\n",
    "    ##########  Code end   ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    #Classifier\n",
    "    def __init__(self, train_data_path, train_labels_path, test_data_path, test_labels_path, layers_arch, learning_rate = 1e-3, batch_size = 32, max_epoch = 200, classes = 10):\n",
    "        self.classes = classes\n",
    "\n",
    "        self.train_data_path = train_data_path\n",
    "        self.train_labels_path = train_labels_path\n",
    "        self.test_data_path = test_data_path\n",
    "        self.test_labels_path = test_labels_path\n",
    "\n",
    "\n",
    "        self.train_data = [] #The shape of train data should be (n_samples,28^2)\n",
    "        self.train_labels = []\n",
    "        self.test_data = []\n",
    "        self.test_labels = []\n",
    "        \n",
    "        self.layers_arch = layers_arch\n",
    "        self.net = Network(layers_arch)\n",
    "        self.loss_function = MSELoss()\n",
    "\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    def dataloader(self):\n",
    "\n",
    "        with open(self.train_data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.train_data.append(np.array(line.strip().split()).astype(np.float64)/255.0)\n",
    "        self.train_data = np.array(self.train_data)\n",
    "\n",
    "        with open(self.train_labels_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.train_labels.append(int(line.strip()))\n",
    "        self.train_labels = np.array(self.train_labels)\n",
    "\n",
    "        with open(self.test_data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.test_data.append(np.array(line.strip().split()).astype(np.float64)/255.0)\n",
    "        self.test_data = np.array(self.test_data)\n",
    "\n",
    "        with open(self.test_labels_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.test_labels.append(int(line.strip()))\n",
    "        self.test_labels = np.array(self.test_labels)\n",
    "\n",
    "\n",
    "    def Train_One_Epoch(self):\n",
    "        '''\n",
    "        Here we train the network using gradient descent\n",
    "        '''\n",
    "        loss = 0\n",
    "        n_loop = int(math.ceil(len(self.train_data) / self.batch_size))\n",
    "        for i in range(n_loop):\n",
    "            batch_data = self.train_data[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            batch_label = self.train_labels[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "            batch_one_hot_label = One_Hot_Encode(batch_label, classes = self.classes)\n",
    "            \n",
    "            '''\n",
    "             /*  Forward the data to the network.\n",
    "             *  Forward the result to the loss function.\n",
    "             *  Backward.\n",
    "             *  Update the weights with weight gradients.\n",
    "             *  Do not forget the learning rate!\n",
    "             */\n",
    "            '''\n",
    "            \n",
    "            ########## Code start  ##########\n",
    "            prediction = self.net.forward(batch_data)\n",
    "            loss += self.loss_function.forward(prediction, batch_one_hot_label)\n",
    "\n",
    "            pred_grad = self.loss_function.backward()\n",
    "            self.net.backward(pred_grad)\n",
    "            for i in range(len(self.layers_arch)):        \n",
    "                if self.layers_arch[i][0] == 'Linear':\n",
    "                    self.net.layers[i].W -= self.net.layers[i].W_grad * self.learning_rate\n",
    "            ##########  Code end   ##########\n",
    "        \n",
    "        return loss / n_loop\n",
    "\n",
    "    def Test(self):\n",
    "        '''\n",
    "        the class with max score is our predicted label\n",
    "        '''\n",
    "        score = self.net.forward(self.test_data)\n",
    "        accuracy = 0\n",
    "        for i in range(np.shape(score)[0]):\n",
    "            one_label_list = score[i].tolist()\n",
    "            label_pred = one_label_list.index(max(one_label_list))\n",
    "            if label_pred == self.test_labels[i]:\n",
    "                accuracy = accuracy +1\n",
    "\n",
    "        accuracy = accuracy/np.shape(score)[0]\n",
    "        return accuracy\n",
    "\n",
    "    def Train(self):\n",
    "        self.dataloader()\n",
    "        x = np.zeros((self.max_epoch, 1))\n",
    "        y = np.zeros((self.max_epoch, 1))\n",
    "        for i in range(self.max_epoch):\n",
    "            loss = self.Train_One_Epoch()\n",
    "            accuray = self.Test()\n",
    "            print(\"Epoch: \", str(i+1), \"/\", str(self.max_epoch), \" | Train loss: \", loss, \" | Test Accuracy : \", accuray)\n",
    "            x[i] = i\n",
    "            y[i] = accuray\n",
    "        plt.figure()\n",
    "        plt.title(\"Classification Network Accuracy vs. Epoch\")\n",
    "        plt.scatter(x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "That's it! Congratulations on finishing everything. Now try your network on MNIST!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 / 200  | Train loss:  0.18642419734127424  | Test Accuracy :  0.1\n",
      "Epoch:  2 / 200  | Train loss:  0.14486545874801454  | Test Accuracy :  0.155\n",
      "Epoch:  3 / 200  | Train loss:  0.1289675991860444  | Test Accuracy :  0.205\n",
      "Epoch:  4 / 200  | Train loss:  0.11745554865987333  | Test Accuracy :  0.27\n",
      "Epoch:  5 / 200  | Train loss:  0.10875800147104996  | Test Accuracy :  0.33\n",
      "Epoch:  6 / 200  | Train loss:  0.10195989232592274  | Test Accuracy :  0.365\n",
      "Epoch:  7 / 200  | Train loss:  0.09648805851834474  | Test Accuracy :  0.4\n",
      "Epoch:  8 / 200  | Train loss:  0.09195942462554793  | Test Accuracy :  0.445\n",
      "Epoch:  9 / 200  | Train loss:  0.08814779851081456  | Test Accuracy :  0.47\n",
      "Epoch:  10 / 200  | Train loss:  0.08488353302653616  | Test Accuracy :  0.51\n",
      "Epoch:  11 / 200  | Train loss:  0.08204846499224552  | Test Accuracy :  0.53\n",
      "Epoch:  12 / 200  | Train loss:  0.07954080539056345  | Test Accuracy :  0.54\n",
      "Epoch:  13 / 200  | Train loss:  0.07730517377895373  | Test Accuracy :  0.55\n",
      "Epoch:  14 / 200  | Train loss:  0.07529770617775526  | Test Accuracy :  0.56\n",
      "Epoch:  15 / 200  | Train loss:  0.07348348994089522  | Test Accuracy :  0.585\n",
      "Epoch:  16 / 200  | Train loss:  0.07182692807820873  | Test Accuracy :  0.59\n",
      "Epoch:  17 / 200  | Train loss:  0.07030522140680764  | Test Accuracy :  0.595\n",
      "Epoch:  18 / 200  | Train loss:  0.0688986120095236  | Test Accuracy :  0.61\n",
      "Epoch:  19 / 200  | Train loss:  0.06759508864633525  | Test Accuracy :  0.615\n",
      "Epoch:  20 / 200  | Train loss:  0.06638168297396661  | Test Accuracy :  0.625\n",
      "Epoch:  21 / 200  | Train loss:  0.06524489361945596  | Test Accuracy :  0.63\n",
      "Epoch:  22 / 200  | Train loss:  0.06417909005999359  | Test Accuracy :  0.645\n",
      "Epoch:  23 / 200  | Train loss:  0.0631748780021935  | Test Accuracy :  0.65\n",
      "Epoch:  24 / 200  | Train loss:  0.062226845821511285  | Test Accuracy :  0.65\n",
      "Epoch:  25 / 200  | Train loss:  0.06132834067082518  | Test Accuracy :  0.65\n",
      "Epoch:  26 / 200  | Train loss:  0.060474802660880594  | Test Accuracy :  0.65\n",
      "Epoch:  27 / 200  | Train loss:  0.0596603377934814  | Test Accuracy :  0.65\n",
      "Epoch:  28 / 200  | Train loss:  0.0588839214766273  | Test Accuracy :  0.655\n",
      "Epoch:  29 / 200  | Train loss:  0.0581427877409017  | Test Accuracy :  0.66\n",
      "Epoch:  30 / 200  | Train loss:  0.05743419251611436  | Test Accuracy :  0.665\n",
      "Epoch:  31 / 200  | Train loss:  0.05675266782292809  | Test Accuracy :  0.67\n",
      "Epoch:  32 / 200  | Train loss:  0.05609833493062063  | Test Accuracy :  0.675\n",
      "Epoch:  33 / 200  | Train loss:  0.0554690404572828  | Test Accuracy :  0.675\n",
      "Epoch:  34 / 200  | Train loss:  0.05486325766849454  | Test Accuracy :  0.675\n",
      "Epoch:  35 / 200  | Train loss:  0.05428110028660456  | Test Accuracy :  0.68\n",
      "Epoch:  36 / 200  | Train loss:  0.05372017828123241  | Test Accuracy :  0.685\n",
      "Epoch:  37 / 200  | Train loss:  0.05317904486887564  | Test Accuracy :  0.685\n",
      "Epoch:  38 / 200  | Train loss:  0.05265426211349961  | Test Accuracy :  0.69\n",
      "Epoch:  39 / 200  | Train loss:  0.05214676808888266  | Test Accuracy :  0.69\n",
      "Epoch:  40 / 200  | Train loss:  0.05165466278414511  | Test Accuracy :  0.705\n",
      "Epoch:  41 / 200  | Train loss:  0.051178432369125476  | Test Accuracy :  0.72\n",
      "Epoch:  42 / 200  | Train loss:  0.05071665599780066  | Test Accuracy :  0.725\n",
      "Epoch:  43 / 200  | Train loss:  0.050268437138719604  | Test Accuracy :  0.725\n",
      "Epoch:  44 / 200  | Train loss:  0.04983323776831939  | Test Accuracy :  0.725\n",
      "Epoch:  45 / 200  | Train loss:  0.0494099704503878  | Test Accuracy :  0.73\n",
      "Epoch:  46 / 200  | Train loss:  0.04899726735297048  | Test Accuracy :  0.73\n",
      "Epoch:  47 / 200  | Train loss:  0.04859521413324102  | Test Accuracy :  0.73\n",
      "Epoch:  48 / 200  | Train loss:  0.0482042255470876  | Test Accuracy :  0.735\n",
      "Epoch:  49 / 200  | Train loss:  0.047822836223645505  | Test Accuracy :  0.735\n",
      "Epoch:  50 / 200  | Train loss:  0.047450685648331575  | Test Accuracy :  0.74\n",
      "Epoch:  51 / 200  | Train loss:  0.047087406566788614  | Test Accuracy :  0.74\n",
      "Epoch:  52 / 200  | Train loss:  0.04673268150393437  | Test Accuracy :  0.745\n",
      "Epoch:  53 / 200  | Train loss:  0.046385068105396596  | Test Accuracy :  0.745\n",
      "Epoch:  54 / 200  | Train loss:  0.04604478843769101  | Test Accuracy :  0.745\n",
      "Epoch:  55 / 200  | Train loss:  0.04571286194644829  | Test Accuracy :  0.745\n",
      "Epoch:  56 / 200  | Train loss:  0.04538674876556288  | Test Accuracy :  0.745\n",
      "Epoch:  57 / 200  | Train loss:  0.045067591218759515  | Test Accuracy :  0.745\n",
      "Epoch:  58 / 200  | Train loss:  0.04475452751247314  | Test Accuracy :  0.745\n",
      "Epoch:  59 / 200  | Train loss:  0.044448469271421646  | Test Accuracy :  0.75\n",
      "Epoch:  60 / 200  | Train loss:  0.04414813529543596  | Test Accuracy :  0.75\n",
      "Epoch:  61 / 200  | Train loss:  0.04385303806492884  | Test Accuracy :  0.75\n",
      "Epoch:  62 / 200  | Train loss:  0.04356430602133698  | Test Accuracy :  0.75\n",
      "Epoch:  63 / 200  | Train loss:  0.04328135449078708  | Test Accuracy :  0.75\n",
      "Epoch:  64 / 200  | Train loss:  0.04300350362782753  | Test Accuracy :  0.75\n",
      "Epoch:  65 / 200  | Train loss:  0.042730816681260476  | Test Accuracy :  0.75\n",
      "Epoch:  66 / 200  | Train loss:  0.04246305064969828  | Test Accuracy :  0.75\n",
      "Epoch:  67 / 200  | Train loss:  0.042199313711541474  | Test Accuracy :  0.75\n",
      "Epoch:  68 / 200  | Train loss:  0.04194080187320799  | Test Accuracy :  0.75\n",
      "Epoch:  69 / 200  | Train loss:  0.0416874708446481  | Test Accuracy :  0.75\n",
      "Epoch:  70 / 200  | Train loss:  0.041438665959264105  | Test Accuracy :  0.755\n",
      "Epoch:  71 / 200  | Train loss:  0.04119350739237718  | Test Accuracy :  0.755\n",
      "Epoch:  72 / 200  | Train loss:  0.040952441267930226  | Test Accuracy :  0.76\n",
      "Epoch:  73 / 200  | Train loss:  0.04071530519364103  | Test Accuracy :  0.765\n",
      "Epoch:  74 / 200  | Train loss:  0.040481677442554334  | Test Accuracy :  0.765\n",
      "Epoch:  75 / 200  | Train loss:  0.040252049795893786  | Test Accuracy :  0.765\n",
      "Epoch:  76 / 200  | Train loss:  0.04002565320324837  | Test Accuracy :  0.77\n",
      "Epoch:  77 / 200  | Train loss:  0.039803647749942356  | Test Accuracy :  0.77\n",
      "Epoch:  78 / 200  | Train loss:  0.03958490219567588  | Test Accuracy :  0.77\n",
      "Epoch:  79 / 200  | Train loss:  0.03936921790749635  | Test Accuracy :  0.775\n",
      "Epoch:  80 / 200  | Train loss:  0.039157312119519755  | Test Accuracy :  0.775\n",
      "Epoch:  81 / 200  | Train loss:  0.03894794962127814  | Test Accuracy :  0.775\n",
      "Epoch:  82 / 200  | Train loss:  0.03874176839663854  | Test Accuracy :  0.775\n",
      "Epoch:  83 / 200  | Train loss:  0.03853760873115222  | Test Accuracy :  0.775\n",
      "Epoch:  84 / 200  | Train loss:  0.03833719874374118  | Test Accuracy :  0.775\n",
      "Epoch:  85 / 200  | Train loss:  0.038139107618910977  | Test Accuracy :  0.775\n",
      "Epoch:  86 / 200  | Train loss:  0.03794422490553289  | Test Accuracy :  0.775\n",
      "Epoch:  87 / 200  | Train loss:  0.037750993170246945  | Test Accuracy :  0.775\n",
      "Epoch:  88 / 200  | Train loss:  0.03756119480186856  | Test Accuracy :  0.775\n",
      "Epoch:  89 / 200  | Train loss:  0.037373556940330455  | Test Accuracy :  0.775\n",
      "Epoch:  90 / 200  | Train loss:  0.03718847149395624  | Test Accuracy :  0.775\n",
      "Epoch:  91 / 200  | Train loss:  0.03700516434535031  | Test Accuracy :  0.775\n",
      "Epoch:  92 / 200  | Train loss:  0.036824576566537336  | Test Accuracy :  0.775\n",
      "Epoch:  93 / 200  | Train loss:  0.036645842018156655  | Test Accuracy :  0.775\n",
      "Epoch:  94 / 200  | Train loss:  0.036469905317485625  | Test Accuracy :  0.775\n",
      "Epoch:  95 / 200  | Train loss:  0.03629630650326319  | Test Accuracy :  0.775\n",
      "Epoch:  96 / 200  | Train loss:  0.036125030184081953  | Test Accuracy :  0.775\n",
      "Epoch:  97 / 200  | Train loss:  0.035955972005274825  | Test Accuracy :  0.775\n",
      "Epoch:  98 / 200  | Train loss:  0.0357894870379114  | Test Accuracy :  0.775\n",
      "Epoch:  99 / 200  | Train loss:  0.03562518879135472  | Test Accuracy :  0.775\n",
      "Epoch:  100 / 200  | Train loss:  0.03546280085532184  | Test Accuracy :  0.775\n",
      "Epoch:  101 / 200  | Train loss:  0.03530244429917008  | Test Accuracy :  0.775\n",
      "Epoch:  102 / 200  | Train loss:  0.03514342692780382  | Test Accuracy :  0.775\n",
      "Epoch:  103 / 200  | Train loss:  0.034986626663092824  | Test Accuracy :  0.775\n",
      "Epoch:  104 / 200  | Train loss:  0.034831605171270374  | Test Accuracy :  0.775\n",
      "Epoch:  105 / 200  | Train loss:  0.03467768421837174  | Test Accuracy :  0.775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  106 / 200  | Train loss:  0.03452610446573466  | Test Accuracy :  0.775\n",
      "Epoch:  107 / 200  | Train loss:  0.03437624549719899  | Test Accuracy :  0.78\n",
      "Epoch:  108 / 200  | Train loss:  0.034227592557169416  | Test Accuracy :  0.78\n",
      "Epoch:  109 / 200  | Train loss:  0.034081484013729006  | Test Accuracy :  0.78\n",
      "Epoch:  110 / 200  | Train loss:  0.033936700868495887  | Test Accuracy :  0.785\n",
      "Epoch:  111 / 200  | Train loss:  0.033793085282482  | Test Accuracy :  0.785\n",
      "Epoch:  112 / 200  | Train loss:  0.033652587286629884  | Test Accuracy :  0.785\n",
      "Epoch:  113 / 200  | Train loss:  0.033512781567944884  | Test Accuracy :  0.785\n",
      "Epoch:  114 / 200  | Train loss:  0.033374627363232934  | Test Accuracy :  0.785\n",
      "Epoch:  115 / 200  | Train loss:  0.033237734166356836  | Test Accuracy :  0.785\n",
      "Epoch:  116 / 200  | Train loss:  0.03310264500409646  | Test Accuracy :  0.785\n",
      "Epoch:  117 / 200  | Train loss:  0.03296841745513099  | Test Accuracy :  0.785\n",
      "Epoch:  118 / 200  | Train loss:  0.032835210320887924  | Test Accuracy :  0.785\n",
      "Epoch:  119 / 200  | Train loss:  0.032703933000981004  | Test Accuracy :  0.79\n",
      "Epoch:  120 / 200  | Train loss:  0.03257343302602169  | Test Accuracy :  0.79\n",
      "Epoch:  121 / 200  | Train loss:  0.03244409981467695  | Test Accuracy :  0.79\n",
      "Epoch:  122 / 200  | Train loss:  0.03231696880040771  | Test Accuracy :  0.79\n",
      "Epoch:  123 / 200  | Train loss:  0.03219049893447649  | Test Accuracy :  0.79\n",
      "Epoch:  124 / 200  | Train loss:  0.032065235322813655  | Test Accuracy :  0.79\n",
      "Epoch:  125 / 200  | Train loss:  0.03194201206432912  | Test Accuracy :  0.79\n",
      "Epoch:  126 / 200  | Train loss:  0.03181946071602862  | Test Accuracy :  0.79\n",
      "Epoch:  127 / 200  | Train loss:  0.03169847240735051  | Test Accuracy :  0.79\n",
      "Epoch:  128 / 200  | Train loss:  0.03157886028790067  | Test Accuracy :  0.795\n",
      "Epoch:  129 / 200  | Train loss:  0.03146069630011346  | Test Accuracy :  0.795\n",
      "Epoch:  130 / 200  | Train loss:  0.03134331418785892  | Test Accuracy :  0.795\n",
      "Epoch:  131 / 200  | Train loss:  0.031227135321335937  | Test Accuracy :  0.795\n",
      "Epoch:  132 / 200  | Train loss:  0.031112207316695634  | Test Accuracy :  0.795\n",
      "Epoch:  133 / 200  | Train loss:  0.030997973288780116  | Test Accuracy :  0.795\n",
      "Epoch:  134 / 200  | Train loss:  0.03088490330890813  | Test Accuracy :  0.795\n",
      "Epoch:  135 / 200  | Train loss:  0.030772221544490923  | Test Accuracy :  0.795\n",
      "Epoch:  136 / 200  | Train loss:  0.03066138127808915  | Test Accuracy :  0.795\n",
      "Epoch:  137 / 200  | Train loss:  0.030551502108361925  | Test Accuracy :  0.795\n",
      "Epoch:  138 / 200  | Train loss:  0.03044250198528445  | Test Accuracy :  0.795\n",
      "Epoch:  139 / 200  | Train loss:  0.030334231709066097  | Test Accuracy :  0.8\n",
      "Epoch:  140 / 200  | Train loss:  0.03022710384450784  | Test Accuracy :  0.8\n",
      "Epoch:  141 / 200  | Train loss:  0.030121220694085158  | Test Accuracy :  0.8\n",
      "Epoch:  142 / 200  | Train loss:  0.030015919810288557  | Test Accuracy :  0.8\n",
      "Epoch:  143 / 200  | Train loss:  0.029911540961893336  | Test Accuracy :  0.8\n",
      "Epoch:  144 / 200  | Train loss:  0.02980864527743695  | Test Accuracy :  0.8\n",
      "Epoch:  145 / 200  | Train loss:  0.02970607456292025  | Test Accuracy :  0.8\n",
      "Epoch:  146 / 200  | Train loss:  0.029604201557728377  | Test Accuracy :  0.8\n",
      "Epoch:  147 / 200  | Train loss:  0.029503622841300883  | Test Accuracy :  0.8\n",
      "Epoch:  148 / 200  | Train loss:  0.0294036131961207  | Test Accuracy :  0.8\n",
      "Epoch:  149 / 200  | Train loss:  0.02930455324430541  | Test Accuracy :  0.8\n",
      "Epoch:  150 / 200  | Train loss:  0.02920640322846181  | Test Accuracy :  0.8\n",
      "Epoch:  151 / 200  | Train loss:  0.02910896427540212  | Test Accuracy :  0.8\n",
      "Epoch:  152 / 200  | Train loss:  0.029012666586280607  | Test Accuracy :  0.8\n",
      "Epoch:  153 / 200  | Train loss:  0.02891669646027584  | Test Accuracy :  0.8\n",
      "Epoch:  154 / 200  | Train loss:  0.02882165140514993  | Test Accuracy :  0.8\n",
      "Epoch:  155 / 200  | Train loss:  0.02872736100678297  | Test Accuracy :  0.8\n",
      "Epoch:  156 / 200  | Train loss:  0.028633053457892504  | Test Accuracy :  0.8\n",
      "Epoch:  157 / 200  | Train loss:  0.028539992655258167  | Test Accuracy :  0.8\n",
      "Epoch:  158 / 200  | Train loss:  0.028447525586459275  | Test Accuracy :  0.8\n",
      "Epoch:  159 / 200  | Train loss:  0.028355458045700388  | Test Accuracy :  0.8\n",
      "Epoch:  160 / 200  | Train loss:  0.028264651957479854  | Test Accuracy :  0.8\n",
      "Epoch:  161 / 200  | Train loss:  0.028174089098332605  | Test Accuracy :  0.8\n",
      "Epoch:  162 / 200  | Train loss:  0.028084843252925373  | Test Accuracy :  0.8\n",
      "Epoch:  163 / 200  | Train loss:  0.02799579581606281  | Test Accuracy :  0.805\n",
      "Epoch:  164 / 200  | Train loss:  0.027907773362429455  | Test Accuracy :  0.805\n",
      "Epoch:  165 / 200  | Train loss:  0.027820217172855644  | Test Accuracy :  0.805\n",
      "Epoch:  166 / 200  | Train loss:  0.02773385488471503  | Test Accuracy :  0.805\n",
      "Epoch:  167 / 200  | Train loss:  0.02764799637003516  | Test Accuracy :  0.805\n",
      "Epoch:  168 / 200  | Train loss:  0.027563068298349584  | Test Accuracy :  0.805\n",
      "Epoch:  169 / 200  | Train loss:  0.027478279944588475  | Test Accuracy :  0.805\n",
      "Epoch:  170 / 200  | Train loss:  0.027394458465784314  | Test Accuracy :  0.805\n",
      "Epoch:  171 / 200  | Train loss:  0.027311162264757567  | Test Accuracy :  0.805\n",
      "Epoch:  172 / 200  | Train loss:  0.027228117137571062  | Test Accuracy :  0.805\n",
      "Epoch:  173 / 200  | Train loss:  0.02714601245359687  | Test Accuracy :  0.805\n",
      "Epoch:  174 / 200  | Train loss:  0.027064526056824193  | Test Accuracy :  0.805\n",
      "Epoch:  175 / 200  | Train loss:  0.02698349413070953  | Test Accuracy :  0.805\n",
      "Epoch:  176 / 200  | Train loss:  0.02690299007558151  | Test Accuracy :  0.805\n",
      "Epoch:  177 / 200  | Train loss:  0.026822942805019682  | Test Accuracy :  0.81\n",
      "Epoch:  178 / 200  | Train loss:  0.026743998555453213  | Test Accuracy :  0.81\n",
      "Epoch:  179 / 200  | Train loss:  0.02666491326748674  | Test Accuracy :  0.81\n",
      "Epoch:  180 / 200  | Train loss:  0.026587059568179392  | Test Accuracy :  0.81\n",
      "Epoch:  181 / 200  | Train loss:  0.02650929346115622  | Test Accuracy :  0.81\n",
      "Epoch:  182 / 200  | Train loss:  0.026431999871605584  | Test Accuracy :  0.81\n",
      "Epoch:  183 / 200  | Train loss:  0.02635562525889918  | Test Accuracy :  0.81\n",
      "Epoch:  184 / 200  | Train loss:  0.02627934917673599  | Test Accuracy :  0.81\n",
      "Epoch:  185 / 200  | Train loss:  0.026203767836527503  | Test Accuracy :  0.81\n",
      "Epoch:  186 / 200  | Train loss:  0.02612855397099852  | Test Accuracy :  0.81\n",
      "Epoch:  187 / 200  | Train loss:  0.02605452965876749  | Test Accuracy :  0.81\n",
      "Epoch:  188 / 200  | Train loss:  0.02598023103275255  | Test Accuracy :  0.81\n",
      "Epoch:  189 / 200  | Train loss:  0.025906872620026476  | Test Accuracy :  0.81\n",
      "Epoch:  190 / 200  | Train loss:  0.02583389283440026  | Test Accuracy :  0.81\n",
      "Epoch:  191 / 200  | Train loss:  0.025761265798822437  | Test Accuracy :  0.81\n",
      "Epoch:  192 / 200  | Train loss:  0.025689047175389224  | Test Accuracy :  0.81\n",
      "Epoch:  193 / 200  | Train loss:  0.025617686921559808  | Test Accuracy :  0.815\n",
      "Epoch:  194 / 200  | Train loss:  0.025546436511540954  | Test Accuracy :  0.815\n",
      "Epoch:  195 / 200  | Train loss:  0.02547554954146474  | Test Accuracy :  0.815\n",
      "Epoch:  196 / 200  | Train loss:  0.025405147230037964  | Test Accuracy :  0.815\n",
      "Epoch:  197 / 200  | Train loss:  0.02533579178448405  | Test Accuracy :  0.815\n",
      "Epoch:  198 / 200  | Train loss:  0.025265905612381866  | Test Accuracy :  0.815\n",
      "Epoch:  199 / 200  | Train loss:  0.025197115542923226  | Test Accuracy :  0.81\n",
      "Epoch:  200 / 200  | Train loss:  0.025128470486150623  | Test Accuracy :  0.81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_path = './MNIST_Sub/train_data.txt'\n",
    "train_labels_path = './MNIST_Sub/train_labels.txt'\n",
    "test_data_path = './MNIST_Sub/test_data.txt'\n",
    "test_labels_path = './MNIST_Sub/test_labels.txt'\n",
    "\n",
    "\n",
    "#classifier\n",
    "classifier_layers_arch = [['Linear', (28*28, 256)], ['ReLU'], ['Linear', (256, 10)]]\n",
    "cls = Classifier(train_data_path, train_labels_path, test_data_path, test_labels_path, layers_arch = classifier_layers_arch, learning_rate = 0.01, batch_size = 32, max_epoch = 200)\n",
    "cls.Train()\n",
    "cls.Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 / 200  | Train loss:  0.152835677185016  | Test Accuracy :  0.17\n",
      "Epoch:  2 / 200  | Train loss:  0.11254809398824757  | Test Accuracy :  0.215\n",
      "Epoch:  3 / 200  | Train loss:  0.10150156579022651  | Test Accuracy :  0.28\n",
      "Epoch:  4 / 200  | Train loss:  0.09367512478927915  | Test Accuracy :  0.315\n",
      "Epoch:  5 / 200  | Train loss:  0.08772090678990269  | Test Accuracy :  0.36\n",
      "Epoch:  6 / 200  | Train loss:  0.08301416766091632  | Test Accuracy :  0.405\n",
      "Epoch:  7 / 200  | Train loss:  0.07915150339816605  | Test Accuracy :  0.43\n",
      "Epoch:  8 / 200  | Train loss:  0.07589744577366032  | Test Accuracy :  0.435\n",
      "Epoch:  9 / 200  | Train loss:  0.07308122689227013  | Test Accuracy :  0.445\n",
      "Epoch:  10 / 200  | Train loss:  0.0706301829368204  | Test Accuracy :  0.49\n",
      "Epoch:  11 / 200  | Train loss:  0.0684593158031548  | Test Accuracy :  0.505\n",
      "Epoch:  12 / 200  | Train loss:  0.06651084946837886  | Test Accuracy :  0.515\n",
      "Epoch:  13 / 200  | Train loss:  0.0647520122036127  | Test Accuracy :  0.52\n",
      "Epoch:  14 / 200  | Train loss:  0.06315365163313376  | Test Accuracy :  0.535\n",
      "Epoch:  15 / 200  | Train loss:  0.061684363442556685  | Test Accuracy :  0.54\n",
      "Epoch:  16 / 200  | Train loss:  0.06032556539695002  | Test Accuracy :  0.55\n",
      "Epoch:  17 / 200  | Train loss:  0.05906083694232473  | Test Accuracy :  0.56\n",
      "Epoch:  18 / 200  | Train loss:  0.05788121794835268  | Test Accuracy :  0.575\n",
      "Epoch:  19 / 200  | Train loss:  0.056778229821813915  | Test Accuracy :  0.575\n",
      "Epoch:  20 / 200  | Train loss:  0.05574050160370256  | Test Accuracy :  0.585\n",
      "Epoch:  21 / 200  | Train loss:  0.05475929594189735  | Test Accuracy :  0.59\n",
      "Epoch:  22 / 200  | Train loss:  0.053829675832481196  | Test Accuracy :  0.595\n",
      "Epoch:  23 / 200  | Train loss:  0.05295001485299936  | Test Accuracy :  0.605\n",
      "Epoch:  24 / 200  | Train loss:  0.052114150201280574  | Test Accuracy :  0.615\n",
      "Epoch:  25 / 200  | Train loss:  0.051315989648901925  | Test Accuracy :  0.63\n",
      "Epoch:  26 / 200  | Train loss:  0.050554316290616165  | Test Accuracy :  0.63\n",
      "Epoch:  27 / 200  | Train loss:  0.049824898899958586  | Test Accuracy :  0.63\n",
      "Epoch:  28 / 200  | Train loss:  0.04912508749978621  | Test Accuracy :  0.635\n",
      "Epoch:  29 / 200  | Train loss:  0.04844944808918721  | Test Accuracy :  0.64\n",
      "Epoch:  30 / 200  | Train loss:  0.047802421455747854  | Test Accuracy :  0.64\n",
      "Epoch:  31 / 200  | Train loss:  0.04717820125416659  | Test Accuracy :  0.64\n",
      "Epoch:  32 / 200  | Train loss:  0.04657781293984046  | Test Accuracy :  0.64\n",
      "Epoch:  33 / 200  | Train loss:  0.046000402471328916  | Test Accuracy :  0.65\n",
      "Epoch:  34 / 200  | Train loss:  0.04544224466697057  | Test Accuracy :  0.655\n",
      "Epoch:  35 / 200  | Train loss:  0.044905516524039714  | Test Accuracy :  0.67\n",
      "Epoch:  36 / 200  | Train loss:  0.044387600265198536  | Test Accuracy :  0.67\n",
      "Epoch:  37 / 200  | Train loss:  0.043883067838706304  | Test Accuracy :  0.67\n",
      "Epoch:  38 / 200  | Train loss:  0.043392715989596035  | Test Accuracy :  0.665\n",
      "Epoch:  39 / 200  | Train loss:  0.04291413461793201  | Test Accuracy :  0.67\n",
      "Epoch:  40 / 200  | Train loss:  0.04245267360151218  | Test Accuracy :  0.67\n",
      "Epoch:  41 / 200  | Train loss:  0.04200132238371107  | Test Accuracy :  0.67\n",
      "Epoch:  42 / 200  | Train loss:  0.041563583443493826  | Test Accuracy :  0.67\n",
      "Epoch:  43 / 200  | Train loss:  0.04113643915493688  | Test Accuracy :  0.675\n",
      "Epoch:  44 / 200  | Train loss:  0.04072164290180675  | Test Accuracy :  0.68\n",
      "Epoch:  45 / 200  | Train loss:  0.040318361165722086  | Test Accuracy :  0.68\n",
      "Epoch:  46 / 200  | Train loss:  0.03992575070915093  | Test Accuracy :  0.685\n",
      "Epoch:  47 / 200  | Train loss:  0.03954381754860406  | Test Accuracy :  0.69\n",
      "Epoch:  48 / 200  | Train loss:  0.03916881387438393  | Test Accuracy :  0.69\n",
      "Epoch:  49 / 200  | Train loss:  0.038803734675060576  | Test Accuracy :  0.69\n",
      "Epoch:  50 / 200  | Train loss:  0.038445265008984895  | Test Accuracy :  0.695\n",
      "Epoch:  51 / 200  | Train loss:  0.038094441642398405  | Test Accuracy :  0.695\n",
      "Epoch:  52 / 200  | Train loss:  0.03775237649846172  | Test Accuracy :  0.7\n",
      "Epoch:  53 / 200  | Train loss:  0.03741866543119334  | Test Accuracy :  0.7\n",
      "Epoch:  54 / 200  | Train loss:  0.037091926921385236  | Test Accuracy :  0.705\n",
      "Epoch:  55 / 200  | Train loss:  0.03677244637659  | Test Accuracy :  0.705\n",
      "Epoch:  56 / 200  | Train loss:  0.036457816477251555  | Test Accuracy :  0.7\n",
      "Epoch:  57 / 200  | Train loss:  0.03614977676620328  | Test Accuracy :  0.71\n",
      "Epoch:  58 / 200  | Train loss:  0.0358481982067175  | Test Accuracy :  0.705\n",
      "Epoch:  59 / 200  | Train loss:  0.03555274548953437  | Test Accuracy :  0.71\n",
      "Epoch:  60 / 200  | Train loss:  0.035264293837721866  | Test Accuracy :  0.715\n",
      "Epoch:  61 / 200  | Train loss:  0.03498059456438939  | Test Accuracy :  0.715\n",
      "Epoch:  62 / 200  | Train loss:  0.03470352214496306  | Test Accuracy :  0.715\n",
      "Epoch:  63 / 200  | Train loss:  0.03443054230878118  | Test Accuracy :  0.715\n",
      "Epoch:  64 / 200  | Train loss:  0.034162988679447566  | Test Accuracy :  0.72\n",
      "Epoch:  65 / 200  | Train loss:  0.033901909660779474  | Test Accuracy :  0.725\n",
      "Epoch:  66 / 200  | Train loss:  0.03364488829699163  | Test Accuracy :  0.725\n",
      "Epoch:  67 / 200  | Train loss:  0.033393045028328155  | Test Accuracy :  0.725\n",
      "Epoch:  68 / 200  | Train loss:  0.03314522499951894  | Test Accuracy :  0.73\n",
      "Epoch:  69 / 200  | Train loss:  0.03290308340331618  | Test Accuracy :  0.73\n",
      "Epoch:  70 / 200  | Train loss:  0.032664657445090006  | Test Accuracy :  0.74\n",
      "Epoch:  71 / 200  | Train loss:  0.032429026688457666  | Test Accuracy :  0.74\n",
      "Epoch:  72 / 200  | Train loss:  0.03219764964472932  | Test Accuracy :  0.74\n",
      "Epoch:  73 / 200  | Train loss:  0.03197048863379501  | Test Accuracy :  0.74\n",
      "Epoch:  74 / 200  | Train loss:  0.031746405499681336  | Test Accuracy :  0.745\n",
      "Epoch:  75 / 200  | Train loss:  0.031526650216698066  | Test Accuracy :  0.745\n",
      "Epoch:  76 / 200  | Train loss:  0.031310964044960705  | Test Accuracy :  0.745\n",
      "Epoch:  77 / 200  | Train loss:  0.031097792729254448  | Test Accuracy :  0.75\n",
      "Epoch:  78 / 200  | Train loss:  0.0308876916890385  | Test Accuracy :  0.75\n",
      "Epoch:  79 / 200  | Train loss:  0.030680968404237917  | Test Accuracy :  0.75\n",
      "Epoch:  80 / 200  | Train loss:  0.030477837161396584  | Test Accuracy :  0.75\n",
      "Epoch:  81 / 200  | Train loss:  0.030276814635619605  | Test Accuracy :  0.75\n",
      "Epoch:  82 / 200  | Train loss:  0.030079242345387593  | Test Accuracy :  0.75\n",
      "Epoch:  83 / 200  | Train loss:  0.029884434002115164  | Test Accuracy :  0.75\n",
      "Epoch:  84 / 200  | Train loss:  0.02969312653272531  | Test Accuracy :  0.75\n",
      "Epoch:  85 / 200  | Train loss:  0.029504309504633265  | Test Accuracy :  0.75\n",
      "Epoch:  86 / 200  | Train loss:  0.02931851563889612  | Test Accuracy :  0.75\n",
      "Epoch:  87 / 200  | Train loss:  0.029135817635873088  | Test Accuracy :  0.755\n",
      "Epoch:  88 / 200  | Train loss:  0.028955081715506932  | Test Accuracy :  0.76\n",
      "Epoch:  89 / 200  | Train loss:  0.028777233644757837  | Test Accuracy :  0.765\n",
      "Epoch:  90 / 200  | Train loss:  0.028601447505054525  | Test Accuracy :  0.765\n",
      "Epoch:  91 / 200  | Train loss:  0.028428749767642415  | Test Accuracy :  0.765\n",
      "Epoch:  92 / 200  | Train loss:  0.028258461415114875  | Test Accuracy :  0.765\n",
      "Epoch:  93 / 200  | Train loss:  0.028089510319428742  | Test Accuracy :  0.765\n",
      "Epoch:  94 / 200  | Train loss:  0.027923141185687925  | Test Accuracy :  0.765\n",
      "Epoch:  95 / 200  | Train loss:  0.027758954290606232  | Test Accuracy :  0.765\n",
      "Epoch:  96 / 200  | Train loss:  0.02759766096778722  | Test Accuracy :  0.765\n",
      "Epoch:  97 / 200  | Train loss:  0.02743793928813651  | Test Accuracy :  0.765\n",
      "Epoch:  98 / 200  | Train loss:  0.02727922369195956  | Test Accuracy :  0.77\n",
      "Epoch:  99 / 200  | Train loss:  0.02712375208059165  | Test Accuracy :  0.77\n",
      "Epoch:  100 / 200  | Train loss:  0.02696930743766401  | Test Accuracy :  0.77\n",
      "Epoch:  101 / 200  | Train loss:  0.02681716078069157  | Test Accuracy :  0.77\n",
      "Epoch:  102 / 200  | Train loss:  0.02666666600540518  | Test Accuracy :  0.775\n",
      "Epoch:  103 / 200  | Train loss:  0.02651753427552376  | Test Accuracy :  0.775\n",
      "Epoch:  104 / 200  | Train loss:  0.026371563714751807  | Test Accuracy :  0.775\n",
      "Epoch:  105 / 200  | Train loss:  0.02622592537344503  | Test Accuracy :  0.775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  106 / 200  | Train loss:  0.026082528292379868  | Test Accuracy :  0.775\n",
      "Epoch:  107 / 200  | Train loss:  0.025940796357139025  | Test Accuracy :  0.775\n",
      "Epoch:  108 / 200  | Train loss:  0.02580094686378668  | Test Accuracy :  0.775\n",
      "Epoch:  109 / 200  | Train loss:  0.025662888492805538  | Test Accuracy :  0.775\n",
      "Epoch:  110 / 200  | Train loss:  0.025526518129806663  | Test Accuracy :  0.775\n",
      "Epoch:  111 / 200  | Train loss:  0.0253911373284471  | Test Accuracy :  0.785\n",
      "Epoch:  112 / 200  | Train loss:  0.02525819667029554  | Test Accuracy :  0.785\n",
      "Epoch:  113 / 200  | Train loss:  0.025125695403920144  | Test Accuracy :  0.785\n",
      "Epoch:  114 / 200  | Train loss:  0.024995706088351492  | Test Accuracy :  0.785\n",
      "Epoch:  115 / 200  | Train loss:  0.02486750731010083  | Test Accuracy :  0.79\n",
      "Epoch:  116 / 200  | Train loss:  0.02474065106077914  | Test Accuracy :  0.795\n",
      "Epoch:  117 / 200  | Train loss:  0.024614651228047357  | Test Accuracy :  0.795\n",
      "Epoch:  118 / 200  | Train loss:  0.024490456815082755  | Test Accuracy :  0.795\n",
      "Epoch:  119 / 200  | Train loss:  0.024367306407055835  | Test Accuracy :  0.795\n",
      "Epoch:  120 / 200  | Train loss:  0.024245754949678896  | Test Accuracy :  0.795\n",
      "Epoch:  121 / 200  | Train loss:  0.024125049529393698  | Test Accuracy :  0.795\n",
      "Epoch:  122 / 200  | Train loss:  0.02400502067219577  | Test Accuracy :  0.795\n",
      "Epoch:  123 / 200  | Train loss:  0.023887457478643695  | Test Accuracy :  0.795\n",
      "Epoch:  124 / 200  | Train loss:  0.023769742321688294  | Test Accuracy :  0.795\n",
      "Epoch:  125 / 200  | Train loss:  0.023655248956262784  | Test Accuracy :  0.795\n",
      "Epoch:  126 / 200  | Train loss:  0.023539695466463277  | Test Accuracy :  0.8\n",
      "Epoch:  127 / 200  | Train loss:  0.023426486417342095  | Test Accuracy :  0.8\n",
      "Epoch:  128 / 200  | Train loss:  0.023313964882455497  | Test Accuracy :  0.8\n",
      "Epoch:  129 / 200  | Train loss:  0.023201979762048016  | Test Accuracy :  0.8\n",
      "Epoch:  130 / 200  | Train loss:  0.023091167897112963  | Test Accuracy :  0.8\n",
      "Epoch:  131 / 200  | Train loss:  0.022982641861312676  | Test Accuracy :  0.805\n",
      "Epoch:  132 / 200  | Train loss:  0.022874441959202574  | Test Accuracy :  0.805\n",
      "Epoch:  133 / 200  | Train loss:  0.022766981710906707  | Test Accuracy :  0.805\n",
      "Epoch:  134 / 200  | Train loss:  0.022660870160816103  | Test Accuracy :  0.805\n",
      "Epoch:  135 / 200  | Train loss:  0.02255548286739776  | Test Accuracy :  0.805\n",
      "Epoch:  136 / 200  | Train loss:  0.022451948270265056  | Test Accuracy :  0.805\n",
      "Epoch:  137 / 200  | Train loss:  0.02234791266110816  | Test Accuracy :  0.805\n",
      "Epoch:  138 / 200  | Train loss:  0.02224542806541634  | Test Accuracy :  0.805\n",
      "Epoch:  139 / 200  | Train loss:  0.0221438920293861  | Test Accuracy :  0.805\n",
      "Epoch:  140 / 200  | Train loss:  0.022043388658999963  | Test Accuracy :  0.805\n",
      "Epoch:  141 / 200  | Train loss:  0.021943602137967692  | Test Accuracy :  0.805\n",
      "Epoch:  142 / 200  | Train loss:  0.021844305041647107  | Test Accuracy :  0.805\n",
      "Epoch:  143 / 200  | Train loss:  0.021746408283757915  | Test Accuracy :  0.805\n",
      "Epoch:  144 / 200  | Train loss:  0.021648968110851874  | Test Accuracy :  0.805\n",
      "Epoch:  145 / 200  | Train loss:  0.02155286817286954  | Test Accuracy :  0.805\n",
      "Epoch:  146 / 200  | Train loss:  0.021456600625432327  | Test Accuracy :  0.805\n",
      "Epoch:  147 / 200  | Train loss:  0.021362716000003994  | Test Accuracy :  0.805\n",
      "Epoch:  148 / 200  | Train loss:  0.021267218869913406  | Test Accuracy :  0.805\n",
      "Epoch:  149 / 200  | Train loss:  0.021174625586406252  | Test Accuracy :  0.805\n",
      "Epoch:  150 / 200  | Train loss:  0.021081273279846935  | Test Accuracy :  0.805\n",
      "Epoch:  151 / 200  | Train loss:  0.020989832522559627  | Test Accuracy :  0.805\n",
      "Epoch:  152 / 200  | Train loss:  0.020898713267933033  | Test Accuracy :  0.805\n",
      "Epoch:  153 / 200  | Train loss:  0.020808421565923874  | Test Accuracy :  0.805\n",
      "Epoch:  154 / 200  | Train loss:  0.020718738498012337  | Test Accuracy :  0.805\n",
      "Epoch:  155 / 200  | Train loss:  0.020629821770480273  | Test Accuracy :  0.805\n",
      "Epoch:  156 / 200  | Train loss:  0.020541758722063843  | Test Accuracy :  0.805\n",
      "Epoch:  157 / 200  | Train loss:  0.020453353909849308  | Test Accuracy :  0.805\n",
      "Epoch:  158 / 200  | Train loss:  0.020367158638560926  | Test Accuracy :  0.805\n",
      "Epoch:  159 / 200  | Train loss:  0.02028059844831931  | Test Accuracy :  0.805\n",
      "Epoch:  160 / 200  | Train loss:  0.020195673268083997  | Test Accuracy :  0.805\n",
      "Epoch:  161 / 200  | Train loss:  0.020110907075313544  | Test Accuracy :  0.805\n",
      "Epoch:  162 / 200  | Train loss:  0.020026463564336686  | Test Accuracy :  0.805\n",
      "Epoch:  163 / 200  | Train loss:  0.019943961824769513  | Test Accuracy :  0.805\n",
      "Epoch:  164 / 200  | Train loss:  0.019860681841053083  | Test Accuracy :  0.805\n",
      "Epoch:  165 / 200  | Train loss:  0.019779596750387256  | Test Accuracy :  0.805\n",
      "Epoch:  166 / 200  | Train loss:  0.019698155630973647  | Test Accuracy :  0.805\n",
      "Epoch:  167 / 200  | Train loss:  0.019617988409684643  | Test Accuracy :  0.805\n",
      "Epoch:  168 / 200  | Train loss:  0.019537555555787782  | Test Accuracy :  0.81\n",
      "Epoch:  169 / 200  | Train loss:  0.019459563473681395  | Test Accuracy :  0.81\n",
      "Epoch:  170 / 200  | Train loss:  0.019380610473889223  | Test Accuracy :  0.815\n",
      "Epoch:  171 / 200  | Train loss:  0.019302994585734656  | Test Accuracy :  0.815\n",
      "Epoch:  172 / 200  | Train loss:  0.019225767320641234  | Test Accuracy :  0.815\n",
      "Epoch:  173 / 200  | Train loss:  0.019149467971668516  | Test Accuracy :  0.815\n",
      "Epoch:  174 / 200  | Train loss:  0.01907288148726988  | Test Accuracy :  0.815\n",
      "Epoch:  175 / 200  | Train loss:  0.018996954806757265  | Test Accuracy :  0.815\n",
      "Epoch:  176 / 200  | Train loss:  0.018921561335766897  | Test Accuracy :  0.815\n",
      "Epoch:  177 / 200  | Train loss:  0.018846997686836207  | Test Accuracy :  0.815\n",
      "Epoch:  178 / 200  | Train loss:  0.018772833128197945  | Test Accuracy :  0.815\n",
      "Epoch:  179 / 200  | Train loss:  0.018699208769625327  | Test Accuracy :  0.815\n",
      "Epoch:  180 / 200  | Train loss:  0.018626024229512404  | Test Accuracy :  0.815\n",
      "Epoch:  181 / 200  | Train loss:  0.0185533636962218  | Test Accuracy :  0.815\n",
      "Epoch:  182 / 200  | Train loss:  0.018480941530303458  | Test Accuracy :  0.815\n",
      "Epoch:  183 / 200  | Train loss:  0.018409036469412124  | Test Accuracy :  0.815\n",
      "Epoch:  184 / 200  | Train loss:  0.01833814389197621  | Test Accuracy :  0.815\n",
      "Epoch:  185 / 200  | Train loss:  0.018267611640117795  | Test Accuracy :  0.815\n",
      "Epoch:  186 / 200  | Train loss:  0.018197001011822284  | Test Accuracy :  0.815\n",
      "Epoch:  187 / 200  | Train loss:  0.01812732723821673  | Test Accuracy :  0.815\n",
      "Epoch:  188 / 200  | Train loss:  0.01805804258179632  | Test Accuracy :  0.815\n",
      "Epoch:  189 / 200  | Train loss:  0.01798904333795204  | Test Accuracy :  0.815\n",
      "Epoch:  190 / 200  | Train loss:  0.017920784937485196  | Test Accuracy :  0.815\n",
      "Epoch:  191 / 200  | Train loss:  0.01785281384533903  | Test Accuracy :  0.815\n",
      "Epoch:  192 / 200  | Train loss:  0.0177852791010926  | Test Accuracy :  0.815\n",
      "Epoch:  193 / 200  | Train loss:  0.017718039773632444  | Test Accuracy :  0.82\n",
      "Epoch:  194 / 200  | Train loss:  0.017651647946397335  | Test Accuracy :  0.82\n",
      "Epoch:  195 / 200  | Train loss:  0.017585360457383795  | Test Accuracy :  0.82\n",
      "Epoch:  196 / 200  | Train loss:  0.017519555365675926  | Test Accuracy :  0.82\n",
      "Epoch:  197 / 200  | Train loss:  0.01745431260287494  | Test Accuracy :  0.82\n",
      "Epoch:  198 / 200  | Train loss:  0.017389524801703426  | Test Accuracy :  0.82\n",
      "Epoch:  199 / 200  | Train loss:  0.017325394449795712  | Test Accuracy :  0.82\n",
      "Epoch:  200 / 200  | Train loss:  0.01726173207559827  | Test Accuracy :  0.82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = './MNIST_Sub/train_data.txt'\n",
    "train_labels_path = './MNIST_Sub/train_labels.txt'\n",
    "test_data_path = './MNIST_Sub/test_data.txt'\n",
    "test_labels_path = './MNIST_Sub/test_labels.txt'\n",
    "\n",
    "\n",
    "#classifier\n",
    "classifier_layers_arch = [['Linear', (28*28, 256)], ['ReLU'], ['Linear', (256, 256)], ['ReLU'], ['Linear', (256, 256)], ['ReLU'], ['Linear', (256, 10)]]\n",
    "# regressor_layers_arch = [['Linear', (1, 16)], ['ReLU'], ['Linear', (16, 16)], ['ReLU'], ['Linear', (16, 1)]]\n",
    "\n",
    "cls = Classifier(train_data_path, train_labels_path, test_data_path, test_labels_path, layers_arch = classifier_layers_arch, learning_rate = 0.01, batch_size = 32, max_epoch = 200)\n",
    "cls.Train()\n",
    "cls.Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
